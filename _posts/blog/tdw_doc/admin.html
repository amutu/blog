<!DOCTYPE html>
<html>
<head>
<title>admin</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">
/* GitHub stylesheet for MarkdownPad (http://markdownpad.com) */
/* Author: Nicolas Hery - http://nicolashery.com */
/* Version: b13fe65ca28d2e568c6ed5d7f06581183df8f2ff */
/* Source: https://github.com/nicolahery/markdownpad-github */

/* RESET
=============================================================================*/

html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, img, ins, kbd, q, s, samp, small, strike, strong, sub, sup, tt, var, b, u, i, center, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td, article, aside, canvas, details, embed, figure, figcaption, footer, header, hgroup, menu, nav, output, ruby, section, summary, time, mark, audio, video {
  margin: 0;
  padding: 0;
  border: 0;
}

/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 20px;
  max-width: 960px;
  margin: 0 auto;
}

body>*:first-child {
  margin-top: 0 !important;
}

body>*:last-child {
  margin-bottom: 0 !important;
}

/* BLOCKS
=============================================================================*/

p, blockquote, ul, ol, dl, table, pre {
  margin: 15px 0;
}

/* HEADERS
=============================================================================*/

h1, h2, h3, h4, h5, h6 {
  margin: 20px 0 10px;
  padding: 0;
  font-weight: bold;
  -webkit-font-smoothing: antialiased;
}

h1 tt, h1 code, h2 tt, h2 code, h3 tt, h3 code, h4 tt, h4 code, h5 tt, h5 code, h6 tt, h6 code {
  font-size: inherit;
}

h1 {
  font-size: 28px;
  color: #000;
}

h2 {
  font-size: 24px;
  border-bottom: 1px solid #ccc;
  color: #000;
}

h3 {
  font-size: 18px;
}

h4 {
  font-size: 16px;
}

h5 {
  font-size: 14px;
}

h6 {
  color: #777;
  font-size: 14px;
}

body>h2:first-child, body>h1:first-child, body>h1:first-child+h2, body>h3:first-child, body>h4:first-child, body>h5:first-child, body>h6:first-child {
  margin-top: 0;
  padding-top: 0;
}

a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 {
  margin-top: 0;
  padding-top: 0;
}

h1+p, h2+p, h3+p, h4+p, h5+p, h6+p {
  margin-top: 10px;
}

/* LINKS
=============================================================================*/

a {
  color: #4183C4;
  text-decoration: none;
}

a:hover {
  text-decoration: underline;
}

/* LISTS
=============================================================================*/

ul, ol {
  padding-left: 30px;
}

ul li > :first-child, 
ol li > :first-child, 
ul li ul:first-of-type, 
ol li ol:first-of-type, 
ul li ol:first-of-type, 
ol li ul:first-of-type {
  margin-top: 0px;
}

ul ul, ul ol, ol ol, ol ul {
  margin-bottom: 0;
}

dl {
  padding: 0;
}

dl dt {
  font-size: 14px;
  font-weight: bold;
  font-style: italic;
  padding: 0;
  margin: 15px 0 5px;
}

dl dt:first-child {
  padding: 0;
}

dl dt>:first-child {
  margin-top: 0px;
}

dl dt>:last-child {
  margin-bottom: 0px;
}

dl dd {
  margin: 0 0 15px;
  padding: 0 15px;
}

dl dd>:first-child {
  margin-top: 0px;
}

dl dd>:last-child {
  margin-bottom: 0px;
}

/* CODE
=============================================================================*/

pre, code, tt {
  font-size: 12px;
  font-family: Consolas, "Liberation Mono", Courier, monospace;
}

code, tt {
  margin: 0 0px;
  padding: 0px 0px;
  white-space: nowrap;
  border: 1px solid #eaeaea;
  background-color: #f8f8f8;
  border-radius: 3px;
}

pre>code {
  margin: 0;
  padding: 0;
  white-space: pre;
  border: none;
  background: transparent;
}

pre {
  background-color: #f8f8f8;
  border: 1px solid #ccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px;
}

pre code, pre tt {
  background-color: transparent;
  border: none;
}

kbd {
    -moz-border-bottom-colors: none;
    -moz-border-left-colors: none;
    -moz-border-right-colors: none;
    -moz-border-top-colors: none;
    background-color: #DDDDDD;
    background-image: linear-gradient(#F1F1F1, #DDDDDD);
    background-repeat: repeat-x;
    border-color: #DDDDDD #CCCCCC #CCCCCC #DDDDDD;
    border-image: none;
    border-radius: 2px 2px 2px 2px;
    border-style: solid;
    border-width: 1px;
    font-family: "Helvetica Neue",Helvetica,Arial,sans-serif;
    line-height: 10px;
    padding: 1px 4px;
}

/* QUOTES
=============================================================================*/

blockquote {
  border-left: 4px solid #DDD;
  padding: 0 15px;
  color: #777;
}

blockquote>:first-child {
  margin-top: 0px;
}

blockquote>:last-child {
  margin-bottom: 0px;
}

/* HORIZONTAL RULES
=============================================================================*/

hr {
  clear: both;
  margin: 15px 0;
  height: 0px;
  overflow: hidden;
  border: none;
  background: transparent;
  border-bottom: 4px solid #ddd;
  padding: 0;
}

/* TABLES
=============================================================================*/

table th {
  font-weight: bold;
}

table th, table td {
  border: 1px solid #ccc;
  padding: 6px 13px;
}

table tr {
  border-top: 1px solid #ccc;
  background-color: #fff;
}

table tr:nth-child(2n) {
  background-color: #f8f8f8;
}

/* IMAGES
=============================================================================*/

img {
  max-width: 100%
}
</style>
</head>
<body>
<p>tdw邮件列表：<a href="https://groups.google.com/d/forum/tdw-user">https://groups.google.com/d/forum/tdw-user</a></p>
<h1>环境依赖</h1>
<ul>
<li><a href="http://www.centos.org/download/">CentOS-x86-64 6.x</a></li>
<li><a href="http://www.oracle.com/technetwork/java/javasebusiness/downloads/java-archive-downloads-javase6-419409.html">Oracle JDK 1.6.x及以上版本</a></li>
<li><a href="http://ant.apache.org/bindownload.cgi">Apache-ant 1.7.x及以上版本</a></li>
<li><a href="http://www.postgresql.org/download/">PostgreSQL 9.2.x及以上版本</a></li>
<li><a href="http://archive.apache.org">Hadoop 2.2.x版本</a></li>
<li><a href="https://code.google.com/p/protobuf/">Protobuf 2.5.0版本</a></li>
<li><a href="https://code.google.com/a/apache-extras.org/p/hadoop-gpl-compression/">hadoop-gpl-compression</a></li>
</ul>
<h1>编译</h1>
<h2>安装c/c++开发环境</h2>
<pre><code>sudo yum groupinstall &quot;Development Tools&quot;
sudo yum install glibc-static wget unzip
</code></pre>

<h2>安装java开发环境</h2>
<p>下载JDK 1.6.x和Apache-ant并安装，设置JAVA_HOME和PATH环境变量，如添加以下到~/.bashrc中（这里将jdk和ant都安装在home中，请根据自己的情况替换相应的路径）： </p>
<pre><code>echo 'export JAVA_HOME=~/jdk_1.6.0_26' &gt;&gt; ~/.bashrc  
echo 'export PATH=$JAVA_HOME/bin:~/apache-ant-1.7.1/bin:$PATH' &gt;&gt; ~/.bashrc
. ~/.bashrc
</code></pre>

<p>检查java与ant环境及版本是否正确安装，运行如下命令检查：</p>
<blockquote>
<p>allison@tdw:~$ ant -version<br />
Apache Ant version 1.7.1 compiled on June 27 2008<br />
allison@tdw:~$ javac -version<br />
javac 1.6.0_26</p>
</blockquote>
<p>如果返回&quot;-bash: xxx: command not found&quot;，或者版本号低于TDW要求，请确认依赖软件是否安装正确，相应的环境变量是否设置生效。</p>
<h2>安装protobuf</h2>
<pre><code>wget https://protobuf.googlecode.com/files/protobuf-2.5.0.zip
unzip ./protobuf-2.5.0.zip
cd ./protobuf-2.5.0
./configure
sudo make install
</code></pre>

<h2>编译TDW QE二进制</h2>
<pre><code>下载编译依赖的LZO压缩库：
wget hadoop-gpl-compression-0.1.0-rc0.tar.gz
tar xf hadoop-gpl-compression-0.1.0-rc0.tar.gz
下载qe.zip到当前目录
unzip ./qe.zip
cp hadoop-gpl-compression-0.1.0/hadoop-gpl-compression-0.1.0.jar qe/lib/
进行编译：
cd qe &amp;&amp; ant package
</code></pre>

<p>看到</p>
<blockquote>
<p>BUILD SUCCESSFUL<br />
Total time: ...  
</p>
</blockquote>
<p>表示编译成功，生成的二进制包在qe/build/dist目录中，有以下几个目录：
- auxlib:运行时辅助lib<br />
- bin：启动、重启脚本<br />
- conf：配置文件所在的目录<br />
- examples：SQL示例<br />
- lib：库目录<br />
- pl：过程语言库及脚本<br />
- PLClient：交互式客户端<br />
- protobuf：运行时protobuf临时目录</p>
<p>qe/build/dist目录用来打包部署生产环境。</p>
<p>编译后dist的代码命名为QE<em>HOME环境变量，后本QE</em>HOME都指向这个目录：  
</p>
<pre><code>export QE_HOME=/home/allison/qe/build/dist  
</code></pre>

<h1>单机环境搭建</h1>
<p>这个章节，我们快速搭建一个单机版的TDW QE环境，并运行简单的SQL进行体验。这个环境中，Hive，元数据，Hadoop都在一台机器上，并且HDFS和MapReduce都是本地运行模式，因此不宜处理较大数据量。</p>
<p>TDW QE安装包中自带了一个hadoop-2.2.0的包在qe/hadoopcore文件夹中，在单机环境中，我们使用这个hadoop在单机环境中运行SQL和MR。解压hadoop-2.2.0的压缩包，然后设置HADOOP_HOME指向这个目录。</p>
<p>export HADOOP_HOME=/home/allison/qe/hadoopcore/hadoop-2.2.0</p>
<h2>默认元数据配置</h2>
<p>首先在本机上要有PostgreSQL服务，使它监听127.0.0.1的5432端口（默认安装和初始化的PostgreSQL即监听127.0.0.1的5432端口），然后使用PG的管理员身份(一般是初始化PG数据库的linux账号,这里是postgres账户)，运行qe/script/tdw<em>meta</em>init.sql脚本，初始化元数据：</p>
<pre><code>psql -h 127.0.0.1 -p 5432 -U postgres postgres -f qe/script/tdw_meta_init.sql
</code></pre>

<p>如果之前初始化过，可以使用PG管理员身份运行qe/script/reset_meta.sql脚本重置元数据，再运行上面的命令重新初始化元数据：</p>
<pre><code>psql -h 127.0.0.1 -p 5432 -U postgres postgres -f qe/script/reset_meta.sql
psql -h 127.0.0.1 -p 5432 -U postgres postgres -f qe/script/tdw_meta_init.sql
</code></pre>

<h2>启动Hive命令行（CLI模式）</h2>
<p>CLI模式区别于hvie server模式，在客户端进行SQL的解析和MR任务的提交。在开发和调试时，使用CLI模式比较方便。</p>
<p>启动hive的命令行：
    cd $QE_HOME/bin &amp;&amp; ./hive -u root -p tdwroot
进入如下命令行界面(TDW QE默认初始化账号root以及它的密码为tdwroot)：</p>
<blockquote>
<p>allison@tdw:~/qe/bin$ ./hive -u root -p tdwroot<br />
session : allison<em>201404021650</em>0.4582392182039239 start!<br />
Connect to TDW successfully!<br />
hive&gt;</p>
</blockquote>
<p>然后就可以运行TDW SQL：</p>
<pre><code>hive&gt; create table tdw_table(key int,value string);
OK
Time taken: 0.574 seconds
hive&gt; show create table tdw_table;
OK
CREATE TABLE tdw_table(
key INT,
value STRING
);
Time taken: 0.113 seconds
hive&gt; insert into tdw_table values(1,'11'),(2,'22'),(3,'haha');
 --...省略日志信息
hive&gt; select * from tdw_table; 
OK
1   11
2   22
3   haha
Time taken: 0.019 seconds
</code></pre>

<h2>启动Hive Server模式</h2>
<p>Hive Server模式是生产环境推荐的使用模式。它的启动方式是： </p>
<pre><code>cd $QE_HOME/bin &amp;&amp; ./start-server.sh  
</code></pre>

<p>这个命令将hive server默认绑定在50000端口。如果希望指定端口，可以将端口号作为第一个参数传入，如：</p>
<pre><code>cd $QE_HOME/bin &amp;&amp; ./start-server.sh 50001
</code></pre>

<p>然后可以通过PLClient进行连接。PLClient的使用方式，请参考下文。</p>
<h1>Hadoop分布式环境配置</h1>
<p>分布式环境与单机环境的最大区别是Hadoop使用local模式，还是分布式模式。在Hadoop生产环境中，TDW QE作为Hadoop的客户端，提交MR任务。因此，需要在TDW QE的机器上部署Hadoop客户端。</p>
<p>然后进行以下初始化：<br />
以hadoop管理员运行以下命令(这里假设TDW QE将以tdwadmin这个hadoop用户提交mr job和做hdfs操作)：</p>
<pre><code>hadoop fs -mkdir /tdwqe/warehouse
hadoop fs -chown -R tdwadmin:tdwadmin /tdwqe/warehouse
hadoop fs -chmod -R 777 /tmp 
</code></pre>

<p>修改TDW QE配置，将$QE_HOME/conf/hive-default.xml中的配置进行修改：
将以下配置项删除：</p>
<pre><code>hadoop.tmp.dir
fs.default.name
mapred.job.tracker
</code></pre>

<p>将以下配置项设置为冒号后的值：</p>
<pre><code>hive.exec.scratchdir:/tmp/tdw-${user.name}
hive.metastore.warehouse.dir:/tdwqe/warehouse
</code></pre>

<p>将HADOOP_HOME环境变量指向Hadoop客户端，如：</p>
<pre><code>export HADOOP_HOME=/home/allison/hadoop-0.20.1
</code></pre>

<p>然后启动TDW QE：</p>
<pre><code>cd $QE_HOME/bin &amp;&amp; ./start-server.sh
</code></pre>

<p>然后就可以通过PLClient连接使用。</p>
<h1>配置元数据分散化</h1>
<p>TDW的元数据支持分散化。元数据database由一个globa database和若干个segment database组成。在使用元数据时，TDW QE对表的qe db名计算一个hash值，将hash值映射在0~9999范围内，通过global元数据database中的一张路由配置信息，确定qe db内的表存储在哪个segment元数据database中。</p>
<p>TDW QE是通过global database中的seg<em>split表中的内容，选择将元数据放到对应的segment database中的。seg</em>split的定义是（可以在qe/script/tdw<em>meta</em>init.sql中找到元数据的定义信息）：</p>
<pre><code>CREATE TABLE seg_split (
    seg_addr character varying NOT NULL,
    seg_accept_range int4range
);
</code></pre>

<p>默认TDW QE的配置如下：</p>
<pre><code>insert into seg_split values('jdbc:postgresql://127.0.0.1:5432/seg_1','[0,10000)');
</code></pre>

<p>表示将所有qe db hash值为0~9999的表，都放在seg<em>1这个元数据database中，seg</em>1的连接地址是jdbc:postgresql://127.0.0.1:5432/seg_1。</p>
<p>如果要将元数据分散在两个segment database中，这两个segment database位于不同的主机上，则可以将上面的insert语句改为：</p>
<pre><code>insert into seg_split values('jdbc:postgresql://192.168.1.2:5432/seg_1','[0,4999)');
insert into seg_split values('jdbc:postgresql://192.168.1.3:5432/seg_2','[5000,10000)');
</code></pre>

<p>除了修改global database中的路由配置信息，还要初始化对应的seg<em>1和seg</em>2数据库，seg<em>1和seg<em>2两个database内的表结构都是一样的，如果他们在不同的PostgreSQL实例中，database名甚至可以相同（如都叫seg</em>1,但是不推荐）。可以通过pg</em>dump -s命令获得一个segment database的schema信息，用这个信息创建其它segment。</p>
<h1>配置多HDFS</h1>
<p>如果想使指定的tdw database数据存储在指定的hdfs中，可以在TDW QE中执行一下操作：</p>
<pre><code>create database abc with(hdfsschema = 'hdfs://tdw-hdfs.tencent-distribute.com:5353');  
</code></pre>

<p>之后，这个db中的所有表的数据，都会存放在hdfsschema指定的hdfs集群中。
如果create database不带with参数，创建的db的存放hdfs是TDW QE运行的机器上hadoop hdfs默认的配置。</p>
<h1>配置pgdata存储引擎</h1>
<p>pgdata存储引擎是将PostgreSQL作为TDW的一个存储引擎，用户在TDW中建表时，如果指定stored as pgdata，那么这张表就会在配置项hive.pgdata.storage.url所指定的PostgreSQL实例中创建，同时在TDW中可见。在PG端，建表所用的用户名和密码，与TDW端的用户名和密码一致，因此在执行建表前，要确保该用户在PG侧账户和密码的存在和正确性。</p>
<p>用户也可以自己指定一个PG实例，以覆盖TDW QE默认的设置。如：</p>
<pre><code>set hive.pgdata.storage.url = jdbc:postgresql://192.168.1.3:5432/mypgdata;
create table test(key int,value string) stored as pgdata;
</code></pre>

<p>pgdata存储引擎的表，在TDW端和PG端都可以操作，数据值保存在PG端。相对于普通的TDW表，pgdata存储引擎表可以有索引，可以快速update、delete，可以使用PG的jdbc、odbc、php等接口进行操作等。而且，对于pgdata存储引擎表，TDW会尽量将过滤条件下推到PG侧，使它在TDW中的访问效率更高。</p>
<h1>PLClient使用</h1>
<p>PLClient提供交互式的查询接口，一下为登陆和运行SQL的演示：</p>
<blockquote>
<p>allison@tdw$ <strong>$QE<em>HOME/bin/plclient/tdwpl</strong><br />
Python version is OK.<br />
No arguments, we fall back to shell mode now ...<br />
Entering shell mode now ...<br />
Got config from cmd line: mode m connect to None:None@None:None<br />
Username: <strong>root</strong><br />
Password: <strong>tdwroot</strong><br />
Use 'connect/c' to connect the server!<br />
TDW/PL Client Version 0.2.3<br />
Auto save config: False<br />
Auto save passwd: False<br />
User home library: '~/.tdwpl'<br />
User: 'root'<br />
Database name 'default</em>db'<br />
Server 'None:None' not connected<br />
Welcome to TDW PL Shell, for help please input ? or help<br />
root@TDW-PL$ <strong>server 127.0.0.1</strong><br />
You have changed server to '127.0.0.1'<br />
root@TDW-PL$ <strong>port 50000</strong><br />
You have changed port to '50000'<br />
root@TDW-PL$ <strong>c</strong><br />
connect to hive ip:127.0.0.1<br />
Connect to server '127.0.0.1:50000' success.<br />
root@TDW-PL$ <strong>new</strong><br />
New Session 6957574802551219 -1485586152<br />
root@TDW-PL$ <strong>create table allison<em>test(key int);</strong><br />
TDW-00000 SUCCEED ( session: 6957574802551219 query: create table allison</em>test(key int) )<br />
root@TDW-PL$ <strong>desc allison<em>test</strong><br />
key     int<br />
TDW-00000 SUCCEED ( session: 6957574802551219 query: desc allison</em>test )<br />
root@TDW-PL$ <strong>help</strong>  
</p>
<pre>
Documented commands (type help <topic>):  
========================================  
EOF            color       ed            kill     put           sleep    
a              compile     edit          ls       q             start    
attach         config      editor        lsl      quit          status  
autoed         connect     exec          lsm      reset         test    
autosaveconf   d           exit          lsu      run           upload  
c              dbname      get           makejar  save          uploadm 
cat            detach      gethistory    n        savepasswd    user    
ci             discard     getqp         new      server      
cl             disconnect  getqueryplan  passwd   serverstatus
clusterstatus  doc         getschema     plc      sethome     
co             downloadm   host          port     shows       
</pre>
</blockquote>
<h1>常用配置说明</h1>
<p>TDW QE的配置文件在$QE<em>HOME/conf目录中，默认有两个,hive-default.xml和hive-log4j.properties,前者是TDW QE的主要配置，后者配置日志行为。hive-default.xml中的配置可以包含Hadoop的配置项，如果包含，会覆盖$HADOOP</em>HOME/conf中的相关配置。</p>
<ul>
<li>
<p>hive.default.fileformat<br />
指定新建表的默认存储格式，当create table没有stored as子句时，将使用改配置指定的存储格式。可选为sequencefile，textfile，rcfile，formatfile.默认为textfile，推荐使用rcfile。</p>
</li>
<li>
<p>hive.default.formatcompress<br />
使用formatfile为默认存储引擎是是否启用LZO压缩，为true时启用LZO压缩，false则不启用。默认为true。</p>
</li>
<li>
<p>hive.service.newlogpath<br />
hive server session log存放路径，每个session会产生一个文件。默认为/tmp/newlog目录。注意要定期清理里面的历史session日志。</p>
</li>
<li>
<p>fetch.execinfo.mode<br />
PLClient在执行完每条SQL时，打印执行过程的统计信息。可选的为no，part，all，打印信息依次增加。默认为part。</p>
</li>
<li>
<p>hive.query.info.log.url</p>
</li>
<li>hive.query.info.log.user</li>
<li>
<p>hive.query.info.log.passwd
TDW QE在Hive Server模式下执行信息上报配置。分别配置为PG的jdbc连接url，连接用户名和密码。url格式如：jdbc:postgresql://192.168.1.3:5432/query<em>info。在源码的script中tdw</em>meta_init.sql中的tdw数据库下的tdw schema可以查看执行信息上报的表结构。注意要定期清理这些表的历史数据。</p>
</li>
<li>
<p>hive.metastore.global.url</p>
</li>
<li>hive.metastore.pbjar.url</li>
<li>hive.metastore.user</li>
<li>
<p>hive.metastore.passwd
hive.metastore.global.url指定元数据global database的jdbc url，格式如：jdbc:postgresql://192.168.1.3:5432/global。hive.metastore.pbjar.url指定protobuf jar包等信息的database usr，格式如：jdbc:postgresql://192.168.1.4:5432/pbjar。
hive.metastore.user和hive.metastore.passwd设置hive.metastore.global.url、hive.metastore.pbjar.url所指定的PostgreSQL元数据库连接的用户名和密码。这个用户名密码，也是TDW QE连接所有segment database的用户名和密码。</p>
</li>
<li>
<p>hive.metastore.warehouse.dir
TDW QE database数据在HDFS上的存放目录。如：/tdwqe/warehouse。</p>
</li>
<li>
<p>hive.exec.scratchdir
TDW QE SQL执行过程中的中间临时数据在HDFS上的存放目录。如/tmp/tdw-${user.name}。</p>
</li>
<li>
<p>hive.pgdata.storage.url
默认的pgdata存储引擎的的PG实例地址，格式如jdbc:postgresql://192.168.1.4:5432/pbdata。</p>
</li>
<li>
<p>hive.pb.badfile.skip</p>
</li>
<li>
<p>hive.pb.badfile.limit
hive.pb.badfile.skip设置是否开启SQL处理的Protobuf文件算坏时，跳过这个文件，而不是立即报错。true表示开启，false表示不开启。默认不开启。hive.pb.badfile.limit设置如果开启跳过PB损坏文件，当损坏文件超过多少时，SQL执行失败。默认是10。</p>
</li>
<li>
<p>hive.max.sql.length
设置TDW QE接受的SQL的最大长度，到超过这个长度时，SQL将报错。这个值大于100时生效。默认是0。</p>
</li>
<li>
<p>hive.session.timeout
设置session多长时间没有活动，Hive Server将其断掉。该值在[0,7200]范围内时，被设置为7200s，否则被设置为60s。默认是0。</p>
</li>
<li>
<p>hive.client.connection.limit</p>
</li>
<li>
<p>hive.client.connection.limit.number
hive.client.connection.limit设置是否开启Hive Server最大连接数限制，true开启，false关闭。默认是true。hive.client.connection.limit.number设置开启Hive Server最大连接数设置时，最大连接数限制值。连接数大于或等于这个值时，Hive Server将决绝连接。默认是50。</p>
</li>
<li>
<p>hive.inputfiles.splitbylinenum</p>
</li>
<li>
<p>hive.inputfiles.line<em>num</em>per<em>split
设置是否开启按行split，以及按照多上行记录，产生一个split。hive.inputfiles.splitbylinenum设置是否开启，默认为false。hive.inputfiles.line<em>num</em>per</em>split设置单个split的行数。只对format和rcfile存储格式生效。</p>
</li>
<li>
<p>hive.exec.parallel</p>
</li>
<li>
<p>hive.exec.parallel.threads
设置是否开启单个SQL无依赖MR并行运行，以及并行的线程数。hive.exec.parallel为true时开启，默认为false。hive.exec.parallel.threads默认是8。</p>
</li>
<li>
<p>hive.multi.rctmpfile.path
SQL在做JOIN等操作时，如果内存不够用，会将数据写入到MR task本地磁盘。这个配置指定多个路径，他们可以分散到多个磁盘，以提高IO效率。配置路径以逗号分隔，且必须是绝对路径。默认是/tmp/data{1..12}。</p>
</li>
</ul>
<h1>ProtoBuf存储格式支持</h1>
<p>TDW支持Protobuf存储格式，用户可以通过proto定义文件，创建TDW表。</p>
<p>下面介绍创建Protobuf表的步骤。</p>
<h2>拷贝proto定义文件</h2>
<p>将proto文件拷贝到$QE_HOME/protobuf/upload/${UserName}/中，UserName为TDW建表用户名。如果一张表对应有多个proto文件，确保将所有的都拷贝到目标目录。</p>
<h2>预处理proto文件</h2>
<p>运行以下命令：</p>
<pre><code>$QE_HOME/bin/makejar.sh pgurl user passwd dbname tablename username filename protoversion 
</code></pre>

<p>参数说明：</p>
<ul>
<li>pgurl，user，passwd：保存jar报的元数据库信息，应该分别与$QE_HOME/conf中的hive.metastore.pbjar.url，hive.metastore.user，hive.metastore.passwd一致。  
</li>
<li>dbname，tablename：要创建表在TDW中的db名和表名，注意表名要与主message名相同。 </li>
<li>username：建表用户名</li>
<li>filename：主proto文件名，这个文件及其包含的文件（如果有的话）应该已经拷贝到指定的目录中。</li>
<li>protoversion：protobuf版本号，目前只支持2.3.0</li>
</ul>
<h2>执行建表语句</h2>
<pre><code>create table tablename partition by list(part_time) (partition default) stored as pb
</code></pre>

<p>tablename：要创建的表名，支持分区表。创建表的dbname，tablename要与预处理中的信息一致。</p>
<h2>注意事项</h2>
<ul>
<li>proto文件名一定要是小写，并且不能包含空格等特殊字符;</li>
<li>proto文件中用到import其他proto文件的，不要写路径，只指明文件名即可，例如import &quot;text.proto&quot;;</li>
<li>主proto文件的message名字一定要与表名相同，根据proto文件生成jar包的时候会进行检查，不相同会报错</li>
<li>自定义的类型名和变量名不能相同（新版本支持区分大小写，即message A类型的变量名可以为a），否则生成jar包会失败</li>
<li>不能包含空的message，否则建表的时候会出错</li>
</ul>
<h1>回归测试</h1>
<p>首先按照“默认元数据配置”章节所描述的方法，初始化元数据，然后执行以下操作：</p>
<pre><code>cd qe
ant test
等待...
ant testreport  
</code></pre>

<p>用浏览器打开qe/build/test/junit-noframes.html可以查看junit测试用例运行情况。<br />
推荐在有4GB以上内存配置的机器上运行回归测试。</p>
<h1>已知问题</h1>
<ol>
<li>
<p>formatfile存储格式单行记录不能超过32KB，rcfile存储格式默认单行记录不能超过4MB，文本存储格式没有这个限制。因此在使用前，请提前做好评估，选择合适的存储格式建表。</p>
</li>
<li>
<p>目前tdw的用户名和密码在元数据库中使明文存储的，没有复杂读要求，并且在传输过程中也是明文传输；TDW默认的密码如tdwroot、元数据默认密码tdwmeta、tdw等请及时修改；hadoop的日志和job配置信息，日志可能带有敏感信息，需要控制访问权限。因此，请在安全可信的环境中（使用防火墙隔离或者物理隔离）使用。在TDW未来版本中，将对安全这块进行改进。</p>
</li>
<li>
<p>在向一个分区表中insert数据时，如果insert的目标分区过大（如数据将insert到超过100个分区），在MR执行过程中可能产生OOM错误，这时请修改SQL逻辑，或者增大MR task的内存配置。</p>
</li>
</ol>
<h1>FAQ</h1>
<ol>
<li>
<p>TDW是什么？<br />
   TDW是腾讯分布式数据仓库（Tencent Distributed Data Warehouse）的简称，是腾讯基于Hive、Hadoop、PostgreSQL等开源软件构建的数据仓库。TDW目前将核心模块--TDW查询引擎（TDW QE）块以apache license 2.0协议开源，这个模块基于apache hive 0.4.1版本进行改造，可以配合社区Hadoop版本快速构建数据仓库系统。</p>
</li>
<li>
<p>TDW QE现在的应用情况是怎样的？<br />
   TDW正式运营已有4年多，目前腾讯内超过90%以上的大数据分析是通过TDW QE的SQL来做的。我们的生产环境有70多个QE（hive server模式）实例在运行，他们共用一套元数据，管理着30万+张表，数千万个分区，近百PB的HDFS数据，每天接受近900万条SQL命令，并发session峰值超过2000。</p>
</li>
<li>
<p>TDW QE相比官方apache hive有什么优势？<br />
   首先TDW QE经过腾讯大规模压力、稳定性、功能上的验证，更稳定；其次，我们有一些社区并不具备的特性：如元数据分散化，多HDFS支持，基于角色的权限管理，range/list分区等；最后我们的文档都是中文，对国内研发人员来说更友好。</p>
</li>
<li>
<p>TDW QE的SQL语法与与apache hive兼容吗？<br />
   我们基于apache hive 0.4.1版本，添加了大量SQL语法和函数，在添加过程中，我们尽量参考已有的SQL语法（如MySQL、PostgreSQL），但是很少参考社区hive 0.4.1后的版本的语法。因此TDW QE的SQL语法与apache hive有差异，不完全兼容。对于SQL语法，TDW QE的往往比apache hive的更标准（如cube和rollup的语法）。</p>
</li>
<li>
<p>与TDW QE配合使用的hadoop版本有哪些？<br />
   在腾讯的TDW生产环境中，我们使用的hadoop版本基于hadoop-2.2.x，目前我们只测试了TDW QE配合hadoop-2.2.x使用，更新版本的hadoop版本我们没有验证。</p>
</li>
</ol>
<h1>TDW相关链接</h1>
<p><a href="http://code.csdn.net/news/2818988">腾讯TDW项目：开源的分布式数据仓库</a><br />
<a href="http://www.csdn.net/article/2012-11-19/2811864-tecent_ZhaoWei_interview">Hive在腾讯分布式数据仓库的实践</a><br />
<a href="http://www.csdn.net/article/a/2012-05-23/2805878">TDW在Hadoop上的实践分享</a><br />
<a href="http://codecloud.net/hadoop-tdw-823.html">大规模Hadoop集群实践：腾讯分布式数据仓库（TDW）</a>  </p>

</body>
</html>
<!-- This document was created with MarkdownPad, the Markdown editor for Windows (http://markdownpad.com) -->
